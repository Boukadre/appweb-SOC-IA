"""
AI Manager - Singleton pour gestion des modÃ¨les IA
Charge les modÃ¨les au dÃ©marrage et les garde en mÃ©moire
"""
from typing import Optional
import torch
from transformers import pipeline
from app.core.config import settings


class AIManager:
    """Singleton pour gÃ©rer les modÃ¨les IA"""
    
    _instance: Optional['AIManager'] = None
    _initialized: bool = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            self.phishing_classifier = None
            self._initialized = True
    
    async def load_models(self):
        """Charge tous les modÃ¨les IA au dÃ©marrage"""
        print("ğŸ”„ Chargement des modÃ¨les IA...")
        
        try:
            # DÃ©terminer le device (CPU/GPU)
            device = -1  # CPU par dÃ©faut
            if settings.AI_DEVICE == "cuda" and torch.cuda.is_available():
                device = 0
                print("ğŸ® GPU CUDA dÃ©tectÃ© - utilisation du GPU")
            elif settings.AI_DEVICE == "mps" and torch.backends.mps.is_available():
                device = 0
                print("ğŸ Apple Silicon (MPS) dÃ©tectÃ©")
            else:
                print("ğŸ’» Utilisation du CPU pour l'infÃ©rence")
            
            # Charger le modÃ¨le de dÃ©tection de phishing
            print(f"ğŸ“¦ Chargement du modÃ¨le: {settings.HF_PHISHING_MODEL}")
            self.phishing_classifier = pipeline(
                "text-classification",
                model=settings.HF_PHISHING_MODEL,
                device=device,
                truncation=True,
                max_length=512
            )
            
            print("âœ… ModÃ¨les IA chargÃ©s avec succÃ¨s")
            print(f"   - Phishing Detection: {settings.HF_PHISHING_MODEL}")
            print(f"   - Device: {'GPU' if device >= 0 else 'CPU'}")
            
        except Exception as e:
            print(f"âš ï¸  Erreur lors du chargement des modÃ¨les IA: {e}")
            print("   Le service fonctionnera en mode dÃ©gradÃ© (heuristique)")
            self.phishing_classifier = None
    
    async def unload_models(self):
        """DÃ©charge les modÃ¨les (appelÃ© Ã  l'arrÃªt)"""
        print("ğŸ”„ DÃ©chargement des modÃ¨les IA...")
        
        if self.phishing_classifier:
            del self.phishing_classifier
            self.phishing_classifier = None
        
        # Force garbage collection
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("âœ… ModÃ¨les IA dÃ©chargÃ©s")
    
    def get_phishing_classifier(self):
        """Retourne le classificateur de phishing"""
        return self.phishing_classifier
    
    def is_available(self) -> bool:
        """VÃ©rifie si les modÃ¨les IA sont disponibles"""
        return self.phishing_classifier is not None


# Instance globale singleton
ai_manager = AIManager()


# Helper functions
async def load_ai_models():
    """Charge les modÃ¨les au dÃ©marrage de l'application"""
    await ai_manager.load_models()


async def unload_ai_models():
    """DÃ©charge les modÃ¨les Ã  l'arrÃªt de l'application"""
    await ai_manager.unload_models()


def get_ai_manager() -> AIManager:
    """Retourne l'instance du AI Manager"""
    return ai_manager



